{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Choosing a U.S. City to Live In To Pursue Data Science Career"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pursuing a Data Science career is an exciting career option for scientists and business people wanting to break into the technology industry.\n",
    "\n",
    "One common consideration when transitioning into a tech career is whether or not it would be beneficial to move into a tech hub such as San Francisco. \n",
    "\n",
    "Although San Francisco is an attractive option, there are other considerations that should be weighed in order to make the best decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate several U.S. cities based on cultural and climatological data to determine which cities would be a good fit to personal preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Weather: A mild weather is preferred.\n",
    "- Scenery: A city near mountains is preferred.\n",
    "- Urbanization and beautification: A city with a large number of parks is preferred.\n",
    "- Outdoors: the availability of hiking trails and outdoor venues is preferred.\n",
    "- Career: tech hub.\n",
    "- Pollen and mold: Lower pollen and mold counts are preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project may be of interest to any person trying to figure out where to move. \n",
    "In order to make an objective, responsible decision, one must research and weigh pros and cons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data will be downloaded and scraped from various sources including Foursquare and U.S. government sites.\n",
    "The Foursquare data will be used to determine how many venues fit in my personal interests per city.\n",
    "The climatological data will be used to record high and low temperature extremes for each city as well as yearly precipitation volume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import libraries needed for this analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: plotly==4.1.0 in /anaconda3/lib/python3.7/site-packages (4.1.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /anaconda3/lib/python3.7/site-packages (from plotly==4.1.0) (1.3.3)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from plotly==4.1.0) (1.12.0)\n",
      "Requirement already satisfied: notebook>=5.3 in /anaconda3/lib/python3.7/site-packages (5.7.8)\n",
      "Requirement already satisfied: ipywidgets>=7.2 in /anaconda3/lib/python3.7/site-packages (7.4.2)\n",
      "Requirement already satisfied: jupyter-core>=4.4.0 in /anaconda3/lib/python3.7/site-packages (from notebook>=5.3) (4.4.0)\n",
      "Requirement already satisfied: terminado>=0.8.1 in /anaconda3/lib/python3.7/site-packages (from notebook>=5.3) (0.8.1)\n",
      "Requirement already satisfied: jinja2 in /anaconda3/lib/python3.7/site-packages (from notebook>=5.3) (2.10)\n",
      "Requirement already satisfied: prometheus-client in /anaconda3/lib/python3.7/site-packages (from notebook>=5.3) (0.6.0)\n",
      "Requirement already satisfied: ipykernel in /anaconda3/lib/python3.7/site-packages (from notebook>=5.3) (5.1.0)\n",
      "Requirement already satisfied: nbformat in /anaconda3/lib/python3.7/site-packages (from notebook>=5.3) (4.4.0)\n",
      "Requirement already satisfied: traitlets>=4.2.1 in /anaconda3/lib/python3.7/site-packages (from notebook>=5.3) (4.3.2)\n",
      "Requirement already satisfied: tornado<7,>=4.1 in /anaconda3/lib/python3.7/site-packages (from notebook>=5.3) (6.0.2)\n",
      "Requirement already satisfied: ipython-genutils in /anaconda3/lib/python3.7/site-packages (from notebook>=5.3) (0.2.0)\n",
      "Requirement already satisfied: nbconvert in /anaconda3/lib/python3.7/site-packages (from notebook>=5.3) (5.4.1)\n",
      "Requirement already satisfied: jupyter-client>=5.2.0 in /anaconda3/lib/python3.7/site-packages (from notebook>=5.3) (5.2.4)\n",
      "Requirement already satisfied: Send2Trash in /anaconda3/lib/python3.7/site-packages (from notebook>=5.3) (1.5.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /anaconda3/lib/python3.7/site-packages (from notebook>=5.3) (18.0.0)\n",
      "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /anaconda3/lib/python3.7/site-packages (from ipywidgets>=7.2) (7.4.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.4.0 in /anaconda3/lib/python3.7/site-packages (from ipywidgets>=7.2) (3.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /anaconda3/lib/python3.7/site-packages (from jinja2->notebook>=5.3) (1.1.1)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /anaconda3/lib/python3.7/site-packages (from nbformat->notebook>=5.3) (3.0.1)\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from traitlets>=4.2.1->notebook>=5.3) (1.12.0)\n",
      "Requirement already satisfied: decorator in /anaconda3/lib/python3.7/site-packages (from traitlets>=4.2.1->notebook>=5.3) (4.4.0)\n",
      "Requirement already satisfied: mistune>=0.8.1 in /anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=5.3) (0.8.4)\n",
      "Requirement already satisfied: pygments in /anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=5.3) (2.3.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=5.3) (0.3)\n",
      "Requirement already satisfied: bleach in /anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=5.3) (3.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=5.3) (1.4.2)\n",
      "Requirement already satisfied: testpath in /anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=5.3) (0.4.2)\n",
      "Requirement already satisfied: defusedxml in /anaconda3/lib/python3.7/site-packages (from nbconvert->notebook>=5.3) (0.5.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /anaconda3/lib/python3.7/site-packages (from jupyter-client>=5.2.0->notebook>=5.3) (2.8.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.2) (0.13.3)\n",
      "Requirement already satisfied: pickleshare in /anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.2) (0.7.5)\n",
      "Requirement already satisfied: backcall in /anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.2) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.2) (4.6.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.2) (2.0.9)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.2) (0.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /anaconda3/lib/python3.7/site-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.2) (40.8.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.3) (19.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /anaconda3/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->notebook>=5.3) (0.14.11)\n",
      "Requirement already satisfied: webencodings in /anaconda3/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=5.3) (0.5.1)\n",
      "Requirement already satisfied: parso>=0.3.0 in /anaconda3/lib/python3.7/site-packages (from jedi>=0.10->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.2) (0.3.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /anaconda3/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.2) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /anaconda3/lib/python3.7/site-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.2) (0.1.7)\n",
      "WARNING conda.base.context:use_only_tar_bz2(632): Conda is constrained to only using the old .tar.bz2 file format because you have conda-build installed, and it is <3.18.3.  Update or remove conda-build to get smaller downloads and faster extractions.\n",
      "Collecting package metadata (repodata.json): / "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "import numpy as np # library to handle data in a vectorized manner\n",
    "import json # library to handle JSON files\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm # Matplotlib and associated plotting modules\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "!pip install plotly==4.1.0\n",
    "!pip install \"notebook>=5.3\" \"ipywidgets>=7.2\"\n",
    "import plotly\n",
    "#import plotly.plotly as py\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "\n",
    "!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "from geopy import Nominatim # convert an address into latitude and longitude values\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "from pandas.io.html import read_html\n",
    "from sklearn.cluster import KMeans # import k-means from clustering stage\n",
    "\n",
    "!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
    "import folium # map rendering library\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a list with cities of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the names of the cities of interest for this analysis\n",
    "cities = ['Houston, TX','Austin, TX','Dallas, TX','San Antonio, TX','Fort Worth, Texas','San Francisco, CA','San Jose, CA','Santa Rosa, CA','Palo Alto, CA','Los Angeles, CA','Santa Barbara, CA',\n",
    "          'San Diego, CA','Long Beach, CA','Palmdale, CA','Bakersfield, CA','Fresno, CA','Seattle, WA','Portland,OR','Miami, FL','Orlando, FL','Atlanta, GA','New Orleans, LA','Grand Junction, CO',\n",
    "          'Denver, CO','Colorado Springs, CO','New York, NY', 'Arlington, VA','Anchorage, AK','Sacramento, CA','Tampa, FL','Des Moines, IA','Reno, NV','Las Vegas, NV',\n",
    "          'Chicago, IL','Detroit, MI','Boston, MA','New Haven, CT', 'St. Louis, MO','Phoenix, AR','Albuquerque, NM','Oklahoma City, OK']\n",
    "\n",
    "len(cities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  Explore Foursquare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browse Foursquare.com and find venue categories of interest.\n",
    "Record Category IDs and order them in thematical lists.\n",
    "\n",
    "In this analysis, venue data is extracted to determine whether one city or another has more venues of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outdoors and Recreation Venues: Trails, Bike Trail, Botanical Gardens, Forest, Mountain, Nature Preserve, National Park\n",
    "\n",
    "outdoors_venues_ID = ['4bf58dd8d48988d159941735','56aa371be4b08b9a8d57355e','52e81612bcbc57f1066b7a22','52e81612bcbc57f1066b7a23','4eb1d4d54b900d56c88a45fc','52e81612bcbc57f1066b7a13','52e81612bcbc57f1066b7a21']\n",
    "                      \n",
    "# Professional & Other Places:  Tech Startup, Convention Center, Observatory\n",
    "\n",
    "professional_venues_ID = ['4bf58dd8d48988d125941735','4bf58dd8d48988d1ff931735','5744ccdfe4b0c0459246b4d9']\n",
    "    \n",
    "#cultural venues:  Spiritual Center: Buddhist Temple, Hindu Temple, Synagoge, Winery\n",
    "\n",
    "cultural_venues_ID = ['52e81612bcbc57f1066b7a3e','52e81612bcbc57f1066b7a3f','4bf58dd8d48988d139941735','4bf58dd8d48988d14b941735']\n",
    "\n",
    "# Food and drink shop: Farmers Market, Health Food Store, Organic Grocery, Fruit and Vegetable Store, Juice Bar\n",
    "\n",
    "food_venues_ID = ['4bf58dd8d48988d1fa941735','50aa9e744b90af0d42d5de0e','52f2ab2ebcbc57f1066b8b45','52f2ab2ebcbc57f1066b8b1c','4bf58dd8d48988d112941735']\n",
    "\n",
    "# Beautification: Park, Flower Shop, Art Studio\n",
    "\n",
    "beauty_venues_ID = ['4bf58dd8d48988d163941735','4bf58dd8d48988d11b951735','58daa1558bbb0b01f18ec1d6']\n",
    "\n",
    "categoryIDs = [outdoors_venues_ID,professional_venues_ID,cultural_venues_ID,food_venues_ID,beauty_venues_ID]\n",
    "\n",
    "print('CategoryID list created.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a Function to Connect and Extract Data from Foursquare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function connects to Foursquare and extracts venues matching a CategoryID and \n",
    "# stores them in the dataframe designated.\n",
    "\n",
    "def getFoursquareCityData(cities, categoryIDs, limit, max_radius):\n",
    "\n",
    "    # Connect to Foursquare and Query each city to find the number of each venue.\n",
    "\n",
    "    client_ID = 'HJQTB2PO3CQ31PY0D3MKAFCODL1XOO2RLY3VXWZ2XVOUHERI'\n",
    "    client_secret = 'YVXU2GICCUXXHV00HDZUG2ZCR5WG50VYWQCCF14A5JJYY31Y'\n",
    "    version = '20180605' # Foursquare API version\n",
    "\n",
    "    print('Your credentails:')\n",
    "    print('CLIENT_ID: ' + client_ID)\n",
    "    print('CLIENT_SECRET:' + client_secret)\n",
    "\n",
    "    venues_list = []\n",
    "    venues_df = pd.DataFrame(columns = ['City','CategoryID','Venue','Latitude','Longitude','Type'])\n",
    "    \n",
    "    for city in cities:\n",
    "        for list in categoryIDs:\n",
    "            for category in list:\n",
    "                url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&near={}&radius={}&limit={}&categoryId={}'.format(\n",
    "                    client_ID,\n",
    "                    client_secret,\n",
    "                    version,\n",
    "                    city,\n",
    "                    max_radius,\n",
    "                    limit,\n",
    "                    category)\n",
    "\n",
    "                city_abr = city.upper()[:3]\n",
    "                try:\n",
    "                    venues = requests.get(url).json()['response']['groups'][0]['items']\n",
    "\n",
    "                    venues_list.append([(\n",
    "                    city,\n",
    "                    category,\n",
    "                    v['venue']['name'], \n",
    "                    v['venue']['location']['lat'], \n",
    "                    v['venue']['location']['lng'],\n",
    "                    v['venue']['categories'][0]['name']) for v in venues])\n",
    "                except IndexError:\n",
    "                    continue\n",
    "                except KeyError:\n",
    "                    continue\n",
    "\n",
    "            venues_df = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n",
    "        print(city_abr + \" records extracted.\")\n",
    "    print(\"The size of your venue dataframe is:\")\n",
    "    print(venues_df.shape)\n",
    "    return venues_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Populate the venues dataframe with the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find venues near each city center around a 50 km radius (around 30 miles). Limit each venue list to 100 distinct venues\n",
    "venues_df = getFoursquareCityData(cities, categoryIDs, 100, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_df.columns = ['City','CategoryID','Venue','Latitude','Longitude','Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. QC and Filter the Foursquare data in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venues_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First observations: \n",
    "- Found out Tree in Foursquare corresponds to tree cutting services - deleted it from categories for final report\n",
    "- Found out duplicates are present\n",
    "- Apparently there is a Hogwarts campus in Austin???\n",
    "- University, Library, Coworking Space as parameters are not as useful as initially we though. Deleted from categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Duplicates!\n",
    "venues_df = venues_df.drop_duplicates()\n",
    "print('This is the size of the dataframe after dropping duplicates')\n",
    "venues_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleted ___ duplicate records!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out which city has the most venues of interest and which one the least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This block of code creates a new dataframe that counts how many total venues are found per city\n",
    "venues_count = pd.DataFrame(venues_df.groupby('City').count()['Venue'])\n",
    "venues_count = venues_count.sort_values(by=['Venue'])\n",
    "venues_count = venues_count.reset_index()\n",
    "max_number = venues_count['Venue'].max()\n",
    "max_city = venues_count.iloc[venues_count['Venue'].idxmax()][0]\n",
    "min_number = venues_count['Venue'].min()\n",
    "min_city = venues_count.iloc[venues_count['Venue'].idxmin()][0]\n",
    "cities = list(venues_count['City'])\n",
    "venues_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The city with the highest amount of venues matching your interests is: ' + str(max_city) +\n",
    "      ' with ' + str(max_number) + ' venues.')\n",
    "print('The city with the lowest amount of venues matching your interests is: ' + str(min_city) +\n",
    "      ' with ' + str(min_number) + ' venues.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out which venue types are more relevant to include in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that counts the total number of venues per type found.\n",
    "venue_types_df = pd.DataFrame(venues_df.groupby('Type').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "venue_types_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that some venues seem less significant than others, so we are going to filter out the least common types \n",
    "and create a list of the relevant types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete types that don't meet a certain thresh\n",
    "\n",
    "relevant_venues_df = venue_types_df[venue_types_df['City'] > 20]\n",
    "relevant_venues_df = relevant_venues_df.reset_index()\n",
    "relevant_venues_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of relevant venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "relevant_types = relevant_venues_df['Type'].tolist()\n",
    "relevant_types\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create final filtered dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out irrelevant types from dataset\n",
    "# Create now the final dataframe where the irrelevant types are \n",
    "df = venues_df\n",
    "df = df.loc[df['Type'].isin(relevant_types)]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Format Foursquare Data for Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for plotting.\n",
    "Create various slices of the data for input in a horizontal bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframes for each venue category\n",
    "\n",
    "outdoors_df = df.loc[df['CategoryID'].isin(outdoors_venues_ID)]\n",
    "startups_df = df.loc[df['CategoryID'].isin(professional_venues_ID)]\n",
    "cultural_df = df.loc[df['CategoryID'].isin(cultural_venues_ID)]\n",
    "food_df = df.loc[df['CategoryID'].isin(food_venues_ID)]\n",
    "beauty_df = df.loc[df['CategoryID'].isin(beauty_venues_ID)]\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the numbers of venues in each category.\n",
    "outdoors_count = pd.DataFrame(outdoors_df.groupby('City').count()['Venue'])\n",
    "outdoors_count = outdoors_count.reindex(venues_count['City'])\n",
    "startups_count = pd.DataFrame(startups_df.groupby('City').count()['Venue'])\n",
    "startups_count = startups_count.reindex(venues_count['City'])\n",
    "cultural_count = pd.DataFrame(cultural_df.groupby('City').count()['Venue'])\n",
    "cultural_count = cultural_count.reindex(venues_count['City'])\n",
    "food_count = pd.DataFrame(food_df.groupby('City').count()['Venue'])\n",
    "food_count = food_count.reindex(venues_count['City'])\n",
    "beauty_count = pd.DataFrame(beauty_df.groupby('City').count()['Venue'])\n",
    "beauty_count = beauty_count.reindex(venues_count['City'])\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to count how many of each category are present per city\n",
    "\n",
    "def makeList(cities, count_df):\n",
    "    N = len(cities)\n",
    "    count_list = []\n",
    "    for index in range(0,N):\n",
    "        city_total = count_df.iloc[index][0]\n",
    "        count_list.append(city_total)\n",
    "    return count_list\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Apply the function to create lists of total venues per category\n",
    "outdoors_count_list = makeList(cities, outdoors_count)\n",
    "startups_count_list = makeList(cities, startups_count)\n",
    "cultural_count_list = makeList(cities, cultural_count)\n",
    "food_count_list = makeList(cities, food_count)\n",
    "beauty_count_list = makeList(cities, beauty_count)\n",
    "print('Done')\n",
    "\n",
    "# YOU HAVE TO ORDER THIS DATAFRAME BY THE ORDER OF VENUE_COUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py.sign_in('tinaprisma','bti38jh0wvCmy3lmrJ95')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Plot Foursquare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "trace1 = go.Bar(\n",
    "    y= cities[0:20],\n",
    "    x= outdoors_count_list[0:20],\n",
    "    name='Outdoors',\n",
    "    orientation = 'h',\n",
    "    marker = dict(\n",
    "        color = 'rgba(73,124,81,0.8)',\n",
    "    )\n",
    ")\n",
    "trace2 = go.Bar(\n",
    "    y= cities[0:20],\n",
    "    x= startups_count_list[0:20],\n",
    "    name='Startups',\n",
    "    orientation = 'h',\n",
    "    marker = dict(\n",
    "        color = 'rgba(81,73,124,0.8)'\n",
    "   )\n",
    ")\n",
    "trace3 = go.Bar(\n",
    "    y= cities[0:20],\n",
    "    x= cultural_count_list[0:20],\n",
    "    name='Cultural',\n",
    "    orientation = 'h',\n",
    "    marker = dict(\n",
    "        color = 'rgba(124,73,116,0.8)',\n",
    "   )\n",
    ")\n",
    "trace4 = go.Bar(\n",
    "    y= cities[0:20],\n",
    "    x= food_count_list[0:20],\n",
    "    name='Food',\n",
    "    orientation = 'h',\n",
    "    marker = dict(\n",
    "        color = 'rgba(139,115,95,0.8)',\n",
    "    )\n",
    ")\n",
    "trace5 = go.Bar(\n",
    "    y= cities[0:20],\n",
    "    x= beauty_count_list[0:20],\n",
    "    name='Beauty',\n",
    "    orientation = 'h',\n",
    "    marker = dict(\n",
    "        color = 'rgba(95,119,139,0.8)',\n",
    "    )\n",
    ")       \n",
    "data = [trace1, trace2, trace3, trace4, trace5]\n",
    "layout = go.Layout(\n",
    "    barmode='stack', title = 'Foursquare Venue Categories in U.S. Cities',xaxis=dict(range=[0, 1800]))\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='marker-h-bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "tracea = go.Bar(\n",
    "    y= cities[20:42],\n",
    "    x= outdoors_count_list[20:42],\n",
    "    name='Outdoors',\n",
    "    orientation = 'h',\n",
    "    marker = dict(\n",
    "        color = 'rgba(73,124,81,0.8)',\n",
    "    )\n",
    ")\n",
    "traceb = go.Bar(\n",
    "    y= cities[20:42],\n",
    "    x= startups_count_list[20:42],\n",
    "    name='Startups',\n",
    "    orientation = 'h',\n",
    "    marker = dict(\n",
    "        color = 'rgba(81,73,124,0.8)'\n",
    "   )\n",
    ")\n",
    "tracec = go.Bar(\n",
    "    y= cities[20:42],\n",
    "    x= cultural_count_list[20:42],\n",
    "    name='Cultural',\n",
    "    orientation = 'h',\n",
    "    marker = dict(\n",
    "        color = 'rgba(124,73,116,0.8)',\n",
    "   )\n",
    ")\n",
    "traced = go.Bar(\n",
    "    y= cities[20:42],\n",
    "    x= food_count_list[20:42],\n",
    "    name='Food',\n",
    "    orientation = 'h',\n",
    "    marker = dict(\n",
    "        color = 'rgba(139,115,95,0.8)',\n",
    "    )\n",
    ")\n",
    "tracee = go.Bar(\n",
    "    y= cities[20:42],\n",
    "    x= beauty_count_list[20:42],\n",
    "    name='Beauty',\n",
    "    orientation = 'h',\n",
    "    marker = dict(\n",
    "        color = 'rgba(95,119,139,0.8)',\n",
    "    )\n",
    ")\n",
    "\n",
    "data2 = [tracea, traceb, tracec, traced, tracee]\n",
    "layout2 = go.Layout(\n",
    "    barmode='stack', title = 'Foursquare Venue Categories in U.S. Cities continued',xaxis=dict(range=[0, 1800])\n",
    ")\n",
    "\n",
    "fig2 = go.Figure(data=data2, layout=layout2)\n",
    "py.iplot(fig2, filename='marker-h2-bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that there are more startups than the limit imposed by Foursquare. WE could add more points inside the city, or we can rely on data points that are more predicitive.\n",
    "According to this analysis, Houston and Denver look very similar, but we know this is not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Scrape Temperature Data for Each City and Compare Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature data was scraped from a US government dataset (NOAA.gov)\n",
    "\n",
    "To do this, we first had to research the city codes for each of our cities.\n",
    "\n",
    "Then, we had to understand the structure of the data as it was published.\n",
    "\n",
    "We found out that there was a pattern in the URLs that we could leverage to scrape automatically lots of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_codes = ['USW00012918','USW00013958','USW00093037','USW00024233','USW00023234','USW00024229']\n",
    "#these correspond to the order found in our list, cities.\n",
    "\n",
    "temp_measures = ['tmin', 'tavg', 'tmax']\n",
    "\n",
    "temperatures_df = pd.DataFrame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a list of URLs where NOAA.gov stores temperature data for the month of August between the years 2000-2019 \n",
    "# August will be a proxy for Summer Temperatures\n",
    "\n",
    "def extractAugURL(city_codes, temp_measures):\n",
    "    \n",
    "    aug_temps_city_list=[]\n",
    "    \n",
    "    for c in range(0,len(city_codes)):\n",
    "        aug_url_list = []\n",
    "        for t in range(0,len(temp_measures)):\n",
    "            aug_url = 'https://www.ncdc.noaa.gov/cag/city/time-series/' + city_codes[c] + '-' + temp_measures[t] + '-1-8-2000-2019.csv' #scrape august data\n",
    "            #aug_url_list.append(aug_url)\n",
    "            aug_temps_city_list.append(aug_url)\n",
    "       \n",
    "    return aug_temps_city_list\n",
    "   \n",
    "# This function creates a list of URLs where NOAA.gov stores temperature data for the month of February between the years 2000-2019 \n",
    "# February will be a proxy for Winter Temperatures\n",
    "\n",
    "def extractFebURL(city_codes, temp_measures):\n",
    "    \n",
    "    feb_temps_city_list=[]\n",
    "    \n",
    "    for c in range(0,len(city_codes)):\n",
    "        feb_url_list = []\n",
    "        for t in range(0,len(temp_measures)):\n",
    "            feb_url = 'https://www.ncdc.noaa.gov/cag/city/time-series/' + city_codes[c] + '-' + temp_measures[t] + '-1-2-2000-2019.csv' #scrape FEBR data\n",
    "            feb_temps_city_list.append(feb_url)\n",
    "        \n",
    "    return feb_temps_city_list\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a list of URLS for each city and season, making use of the above functions\n",
    "aug_urls = extractAugURL(city_codes, temp_measures)\n",
    "feb_urls = extractFebURL(city_codes, temp_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the August Temperature Data CSV file from NOAA.gov and store it as a dataframe\n",
    "# Create a list of dataframes that will later be filtered for each City\n",
    "aug_master_list = []  # created a list with aaaallll the dataframes \n",
    "for url in range(0,len(aug_urls)):\n",
    "    aug_df = pd.read_csv(aug_urls[url])\n",
    "    aug_df = aug_df.drop([0,1,2])\n",
    "    aug_df = aug_df.reset_index()\n",
    "    aug_df = aug_df.drop(aug_df.columns[-1], axis=1)\n",
    "    aug_df = aug_df.drop(aug_df.columns[-1], axis=1)\n",
    "    aug_df = aug_df.drop(aug_df.columns[0], axis=1)\n",
    "    aug_master_list.append(aug_df)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extract the February Temperature Data CSV file from NOAA.gov and store it as a dataframe\n",
    "# Create a list of dataframes that will later be filtered for each City\n",
    "\n",
    "feb_master_list = []  \n",
    "for url in range(0,len(feb_urls)):\n",
    "    feb_df = pd.read_csv(feb_urls[url])\n",
    "    feb_df = feb_df.drop([0,1,2])\n",
    "    feb_df = feb_df.reset_index()\n",
    "    feb_df = feb_df.drop(feb_df.columns[-1], axis=1)\n",
    "    feb_df = feb_df.drop(feb_df.columns[-1], axis=1)\n",
    "    feb_df = feb_df.drop(feb_df.columns[0], axis=1)\n",
    "    feb_master_list.append(feb_df)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a dataframe for each US City analysed containing organized temperature data\n",
    "\n",
    "def createCityTempDF(feb_master_list, aug_master_list, start_index):\n",
    "    df = pd.DataFrame()\n",
    "    list1=[]\n",
    "    list1a=[]\n",
    "    list2=[]\n",
    "    list3=[]\n",
    "    list4=[]\n",
    "    list5=[]\n",
    "    list6=[]\n",
    "    i1 = start_index\n",
    "    i2 = start_index + 1\n",
    "    i3 = start_index + 2\n",
    "    for num in range(0,19):\n",
    "        df1 = feb_master_list[i1]\n",
    "        df2 = feb_master_list[i2]\n",
    "        df3 = feb_master_list[i3]\n",
    "        df4 = aug_master_list[i1]\n",
    "        df5 = aug_master_list[i2]\n",
    "        df6 = aug_master_list[i3]\n",
    "        list1.append(df1.iloc[num][0])\n",
    "        list1a.append(df1.iloc[num][1])\n",
    "        list2.append(df2.iloc[num][1])\n",
    "        list3.append(df3.iloc[num][1])\n",
    "        list4.append(df4.iloc[num][1])\n",
    "        list5.append(df5.iloc[num][1])\n",
    "        list6.append(df6.iloc[num][1])\n",
    "    df['Date'] = pd.Series(list1, index = df1.index[:len(list1)])\n",
    "    df['Feb Minimum'] = pd.Series(list1a, index = df1.index[:len(list2)]) \n",
    "    df['Feb Average'] = pd.Series(list2, index = df2.index[:len(list3)])\n",
    "    df['Feb Maximum'] = pd.Series(list3, index = df3.index[:len(list4)]) \n",
    "    df['Aug Minimum'] = pd.Series(list4, index = df4.index[:len(list2)]) \n",
    "    df['Aug Average'] = pd.Series(list5, index = df5.index[:len(list3)])\n",
    "    df['Aug Maximum'] = pd.Series(list6, index = df6.index[:len(list4)])\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Temperature Data for Each City and Store in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hou_temp_df = createCityTempDF(feb_master_list, aug_master_list, 0)\n",
    "aus_temp_df = createCityTempDF(feb_master_list, aug_master_list, 3)\n",
    "den_temp_df = createCityTempDF(feb_master_list, aug_master_list, 6)\n",
    "sea_temp_df = createCityTempDF(feb_master_list, aug_master_list, 9)\n",
    "san_temp_df = createCityTempDF(feb_master_list, aug_master_list, 12)\n",
    "por_temp_df = createCityTempDF(feb_master_list, aug_master_list, 15)\n",
    "print('This is an example of the resulting dataframe:')\n",
    "por_temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Plot Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store Temperatures in variables in preparation for the plot.\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "x = list(range(0,19))\n",
    "x_rev = x[::-1]\n",
    "\n",
    "# Houston Summer Temperatures\n",
    "y1 = list(hou_temp_df['Aug Average'])\n",
    "y1_upper = list(hou_temp_df['Aug Maximum'])\n",
    "y1_lower = list(hou_temp_df['Aug Minimum'])\n",
    "y1_lower = y1_lower[::-1]\n",
    "\n",
    "# Houston Summer Avg T\n",
    "trace1 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y1,\n",
    "    line=dict(color='rgb(73,124,81)'),\n",
    "    mode='lines',\n",
    "    name='°F (Summer)',\n",
    "    showlegend=False\n",
    ")\n",
    "# Houston Summer Hi & Lo T\n",
    "trace2 = go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y1_upper+y1_lower,\n",
    "    fill='tozerox',\n",
    "    fillcolor='rgba(73,124,81,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    "    hoverinfo='none'\n",
    ")\n",
    "\n",
    "# Houston Winter Temperatures \n",
    "y2 = list(hou_temp_df['Feb Average'])\n",
    "y2_upper = list(hou_temp_df['Feb Maximum'])\n",
    "y2_lower = list(hou_temp_df['Feb Minimum'])\n",
    "y2_lower = y2_lower[::-1]\n",
    "\n",
    "# Houston Winter Line Average\n",
    "trace3 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y2,\n",
    "    line=dict(color='rgb(73,124,81)'),\n",
    "    mode='lines',\n",
    "    name='°F (Winter)',\n",
    "    showlegend=False,\n",
    ")\n",
    "# Houston Winter High and Low\n",
    "trace4 = go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y2_upper+y2_lower,\n",
    "    fill='tozerox',\n",
    "    fillcolor='rgba(73,124,81,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    "    hoverinfo='none'\n",
    ")\n",
    "\n",
    "# Austin Summer Temperatures\n",
    "y3 = list(aus_temp_df['Aug Average'])\n",
    "y3_upper = list(aus_temp_df['Aug Maximum'])\n",
    "y3_lower = list(aus_temp_df['Aug Minimum'])\n",
    "y3_lower = y3_lower[::-1]\n",
    "\n",
    "# Austin Summer Avg T\n",
    "trace5 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y3,\n",
    "    line=dict(color='rgb(81,73,124)'),\n",
    "    mode='lines',\n",
    "    name='°F (Summer)',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Austin Summer Hi & Lo T\n",
    "trace6 = go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y3_upper+y3_lower,\n",
    "    fill='tozerox',\n",
    "    fillcolor='rgba(81,73,124,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    "    hoverinfo='none'\n",
    ")\n",
    "\n",
    "# Austin Winter Temperatures\n",
    "y4 = list(aus_temp_df['Feb Average'])\n",
    "y4_upper = list(aus_temp_df['Feb Maximum'])\n",
    "y4_lower = list(aus_temp_df['Feb Minimum'])\n",
    "y4_lower = y4_lower[::-1]\n",
    "\n",
    "# Austin Winter Average T\n",
    "trace7 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y4,\n",
    "    line=dict(color='rgb(81,73,124)'),\n",
    "    mode='lines',\n",
    "    name='°F (Winter)',\n",
    "    showlegend=False,\n",
    ")\n",
    "# Austin Winter Hi & Lo T\n",
    "trace8 = go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y4_upper+y4_lower,\n",
    "    fill='tozerox',\n",
    "    fillcolor='rgba(81,73,124,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    "    hoverinfo='none'\n",
    ")\n",
    "# Denver Summer Temperatures\n",
    "y5 = list(den_temp_df['Aug Average'])\n",
    "y5_upper = list(den_temp_df['Aug Maximum'])\n",
    "y5_lower = list(den_temp_df['Aug Minimum'])\n",
    "y5_lower = y5_lower[::-1]\n",
    "\n",
    "# Denver Summer Average T\n",
    "trace9 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y5,\n",
    "    line=dict(color='rgb(124,73,116)'),\n",
    "    mode='lines',\n",
    "    name='°F (Summer)',\n",
    "    showlegend=False\n",
    ")\n",
    "# Denver Summer Hi & Lo T\n",
    "trace10 = go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y5_upper+y5_lower,\n",
    "    fill='tozerox',\n",
    "    fillcolor='rgba(124,73,116,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    "    hoverinfo='none'\n",
    ")\n",
    "\n",
    "# Denver Winter Temperatures\n",
    "y6 = list(den_temp_df['Feb Average'])\n",
    "y6_upper = list(den_temp_df['Feb Maximum'])\n",
    "y6_lower = list(den_temp_df['Feb Minimum'])\n",
    "y6_lower = y6_lower[::-1]\n",
    "\n",
    "# Denver Winter Average T\n",
    "trace11 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y6,\n",
    "    line=dict(color='rgb(124,73,116)'),\n",
    "    mode='lines',\n",
    "    name='°F (Winter)',\n",
    "    showlegend=False,\n",
    ")\n",
    "# Denver Winter Hi & Lo T\n",
    "trace12 = go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y6_upper+y6_lower,\n",
    "    fill='tozerox',\n",
    "    fillcolor='rgba(124,73,116,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    "    hoverinfo='none'\n",
    ")\n",
    "\n",
    "\n",
    "# Seattle Summer Temperature\n",
    "y7 = list(sea_temp_df['Aug Average'])\n",
    "y7_upper = list(sea_temp_df['Aug Maximum'])\n",
    "y7_lower = list(sea_temp_df['Aug Minimum'])\n",
    "y7_lower = y7_lower[::-1]\n",
    "\n",
    "# Seattle Summer Average T\n",
    "trace13 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y7,\n",
    "    line=dict(color='rgb(124,73,116)'),\n",
    "    mode='lines',\n",
    "    name='°F (Summer)',\n",
    "    showlegend=False\n",
    ")\n",
    "# Seattle Summer Hi & Lo T\n",
    "trace14 = go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y7_upper+y7_lower,\n",
    "    fill='tozerox',\n",
    "    fillcolor='rgba(124,73,116,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    "    hoverinfo='none'\n",
    ")\n",
    "\n",
    "# Seattle Winter Temperatures\n",
    "y8 = list(sea_temp_df['Feb Average'])\n",
    "y8_upper = list(sea_temp_df['Feb Maximum'])\n",
    "y8_lower = list(sea_temp_df['Feb Minimum'])\n",
    "y8_lower = y8_lower[::-1]\n",
    "\n",
    "# Seattle Winter Average T\n",
    "trace15 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y8,\n",
    "    line=dict(color='rgb(124,73,116)'),\n",
    "    mode='lines',\n",
    "    name='°F (Winter)',\n",
    "    showlegend=False,\n",
    ")\n",
    "# Seattle Winter Hi & Lo T\n",
    "trace16 = go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y8_upper+y8_lower,\n",
    "    fill='tozerox',\n",
    "    fillcolor='rgba(124,73,116,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    "    hoverinfo='none'\n",
    ")\n",
    "# San Francisco Summer Temperatures\n",
    "y9 = list(san_temp_df['Aug Average'])\n",
    "y9_upper = list(san_temp_df['Aug Maximum'])\n",
    "y9_lower = list(san_temp_df['Aug Minimum'])\n",
    "y9_lower = y9_lower[::-1]\n",
    "\n",
    "# San Francisco Summer Average T\n",
    "trace17 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y9,\n",
    "    line=dict(color='rgb(139,115,95)'),\n",
    "    mode='lines',\n",
    "    name='°F (Summer)',\n",
    "    showlegend=False\n",
    ")\n",
    "# San Francisco Summer Hi & Lo T\n",
    "trace18 = go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y9_upper+y9_lower,\n",
    "    fill='tozerox',\n",
    "    fillcolor='rgba(139,115,95,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    "    hoverinfo='none'\n",
    ")\n",
    "\n",
    "# San Francisco Winter Temperatures\n",
    "y10 = list(san_temp_df['Feb Average'])\n",
    "y10_upper = list(san_temp_df['Feb Maximum'])\n",
    "y10_lower = list(san_temp_df['Feb Minimum'])\n",
    "y10_lower = y10_lower[::-1]\n",
    "\n",
    "# San Francisco Winter Average T\n",
    "trace19 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y10,\n",
    "    line=dict(color='rgb(139,115,95)'),\n",
    "    mode='lines',\n",
    "    name='°F (Winter)',\n",
    "    showlegend=False,\n",
    ")\n",
    "# San Francisco Winter Hi & Lo T\n",
    "trace20 = go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y10_upper+y10_lower,\n",
    "    fill='tozerox',\n",
    "    fillcolor='rgba(139,115,95,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    "    hoverinfo='none'\n",
    ")\n",
    "\n",
    "# Portland Summer Temperatures\n",
    "y11= list(por_temp_df['Aug Average'])\n",
    "y11_upper = list(por_temp_df['Aug Maximum'])\n",
    "y11_lower = list(por_temp_df['Aug Minimum'])\n",
    "y11_lower = y11_lower[::-1]\n",
    "\n",
    "# Portland Summer Average T\n",
    "trace21 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y11,\n",
    "    line=dict(color='rgb(95,119,139)'),\n",
    "    mode='lines',\n",
    "    name='°F (Summer)',\n",
    "    showlegend=False\n",
    ")\n",
    "# Portland Summer Hi & Lo T\n",
    "trace22 = go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y11_upper+y11_lower,\n",
    "    fill='tozerox',\n",
    "    fillcolor='rgba(95,119,139,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    "    hoverinfo='none'\n",
    ")\n",
    "# Portland Winter Temperatures\n",
    "y12 = list(por_temp_df['Feb Average'])\n",
    "y12_upper = list(por_temp_df['Feb Maximum'])\n",
    "y12_lower = list(por_temp_df['Feb Minimum'])\n",
    "y12_lower = y12_lower[::-1]\n",
    "\n",
    "# Portland Winter Average T\n",
    "trace23 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y12,\n",
    "    line=dict(color='rgb(95,119,139)'),\n",
    "    mode='lines',\n",
    "    name='°F (Winter)',\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "# Portland Winter Hi and Lo T\n",
    "trace24 = go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y=y12_upper+y12_lower,\n",
    "    fill='tozerox',\n",
    "    fillcolor='rgba(95,119,139,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    "    hoverinfo='none'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the layout and append lines for plotting\n",
    "\n",
    "data = [trace1, trace2, trace3, trace4, trace5, trace6, trace7, trace8, trace9, trace10, trace11, trace12,\n",
    "        trace13, trace14, trace15, trace16, trace17, trace18, trace19, trace20, trace21, trace22, trace23, trace24]\n",
    "\n",
    "\n",
    "fig = tools.make_subplots(rows=3, cols=2, subplot_titles=('Houston', 'Austin',\n",
    "                                                          'Denver', 'Seattle',\n",
    "                                                          'San Francisco','Portland'))\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 1)\n",
    "fig.append_trace(trace3, 1, 1)\n",
    "fig.append_trace(trace4, 1, 1)\n",
    "fig.append_trace(trace5, 1, 2)\n",
    "fig.append_trace(trace6, 1, 2)\n",
    "fig.append_trace(trace7, 1, 2)\n",
    "fig.append_trace(trace8, 1, 2)\n",
    "fig.append_trace(trace9, 2, 1)\n",
    "fig.append_trace(trace10, 2, 1)\n",
    "fig.append_trace(trace11, 2, 1)\n",
    "fig.append_trace(trace12, 2, 1)\n",
    "fig.append_trace(trace13, 2, 2)\n",
    "fig.append_trace(trace14, 2, 2)\n",
    "fig.append_trace(trace15, 2, 2)\n",
    "fig.append_trace(trace16, 2, 2)\n",
    "fig.append_trace(trace17, 3, 1)\n",
    "fig.append_trace(trace18, 3, 1)\n",
    "fig.append_trace(trace19, 3, 1)\n",
    "fig.append_trace(trace20, 3, 1)\n",
    "fig.append_trace(trace21, 3, 2)\n",
    "fig.append_trace(trace22, 3, 2)\n",
    "fig.append_trace(trace23, 3, 2)\n",
    "fig.append_trace(trace24, 3, 2)\n",
    "\n",
    "fig['layout'].update(title='Summer & Winter Temperatures in U.S. Cities (2000-2018)')\n",
    "\n",
    "py.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Scrape Precipitation Data and Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates URLs used to scrape data automatically\n",
    "def extractRainURL(city_codes):\n",
    "    \n",
    "    precipitation_list = []\n",
    "    \n",
    "    for c in range(0,len(city_codes)):\n",
    "        \n",
    "        pre_url = 'https://www.ncdc.noaa.gov/cag/city/time-series/' + city_codes[c] + '-pcp-12-12-2000-2019.csv' #scrape august data\n",
    "            #aug_url_list.append(aug_url)\n",
    "        precipitation_list.append(pre_url)\n",
    "       \n",
    "    return precipitation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block extracts the precipitation data CSV files from NOAA.gov and creates a list of dataframes\n",
    "precipitation_urls = extractRainURL(city_codes)\n",
    "precipitation_master_list = []  # created a list with aaaallll the dataframes \n",
    "for url in range(0,len(precipitation_urls)):\n",
    "    pre_df = pd.read_csv(precipitation_urls[url])\n",
    "    pre_df = pre_df.drop([0,1,2])\n",
    "    pre_df = pre_df.reset_index()\n",
    "    pre_df = pre_df.drop(pre_df.columns[-1], axis=1)\n",
    "    pre_df = pre_df.drop(pre_df.columns[-1], axis=1)\n",
    "    pre_df = pre_df.drop(pre_df.columns[0], axis=1)\n",
    "    precipitation_master_list.append(pre_df)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function creates a dataframe of precipitation data for a city at the \"start index\"\n",
    "def createCityTempDF(precipitation_master_list, start_index):\n",
    "    df = pd.DataFrame()\n",
    "    list=[]\n",
    "    i1 = start_index\n",
    "    for num in range(0,19):\n",
    "        df = precipitation_master_list[i1]\n",
    "        list.append(df.iloc[num][1])\n",
    "    df['Precipitation'] = pd.Series(list, index = df.index[:len(list)])\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation_master_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a dataframe of precipitation data for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hou_prec_df = createCityTempDF(precipitation_master_list, 0)\n",
    "aus_prec_df = createCityTempDF(precipitation_master_list, 1)\n",
    "den_prec_df = createCityTempDF(precipitation_master_list, 2)\n",
    "sea_prec_df = createCityTempDF(precipitation_master_list, 3)\n",
    "san_prec_df = createCityTempDF(precipitation_master_list, 4)\n",
    "por_prec_df = createCityTempDF(precipitation_master_list, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Precipation Data for Each City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Store precipitation data in variables in preparation for the plot.\n",
    "\n",
    "x = list(range(0,19))\n",
    "x_rev = x[::-1]\n",
    "\n",
    "# Houston Prec Data 2000 - 2019\n",
    "y1 = list(hou_prec_df['Precipitation'])\n",
    "y1_lower = [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "y1_lower = y1_lower[::-1]\n",
    "trace1 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y1,\n",
    "    line=dict(color='rgb(81,73,124)'),\n",
    "    mode='lines',\n",
    "    name='Inches',\n",
    "    showlegend=False,\n",
    "\n",
    ")\n",
    "'''trace7 = go.Scatter(\n",
    "    x=x+x_rev,\n",
    "    y= y1,\n",
    "    fill='tozerox',\n",
    "    fillcolor='rgba(81,73,124,0.2)',\n",
    "    line=dict(color='rgba(255,255,255,0)'),\n",
    "    showlegend=False,\n",
    ")'''\n",
    "y2 = list(aus_prec_df['Precipitation'])\n",
    "trace2 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y2,\n",
    "    line=dict(color='rgb(124,73,116)'),\n",
    "    mode='lines',\n",
    "    name='Inches',\n",
    "    showlegend=False\n",
    ")\n",
    "y3 = list(den_prec_df['Precipitation'])\n",
    "trace3 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y3,\n",
    "    line=dict(color='rgb(139,115,95)'),\n",
    "    mode='lines',\n",
    "    name='Inches',\n",
    "    showlegend=False\n",
    ")\n",
    "y4 = list(sea_prec_df['Precipitation'])\n",
    "trace4 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y4,\n",
    "    line=dict(color='rgb(95,119,139)'),\n",
    "    mode='lines',\n",
    "    name='Inches',\n",
    "    showlegend=False\n",
    ")\n",
    "y5 = list(san_prec_df['Precipitation'])\n",
    "trace5 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y5,\n",
    "    line=dict(color='rgb(110, 115, 119)'),\n",
    "    mode='lines',\n",
    "    name='Inches',\n",
    "    showlegend=False\n",
    ")\n",
    "y6 = list(san_prec_df['Precipitation'])\n",
    "trace6 = go.Scatter(\n",
    "    x=x,\n",
    "    y=y6,\n",
    "    line=dict(color='rgb(95,119,139)'),\n",
    "    mode='lines',\n",
    "    name='Inches',\n",
    "    showlegend=False\n",
    ")\n",
    "data = [trace1, trace2, trace3, trace4, trace5, trace6] #trace7]\n",
    "\n",
    "fig = tools.make_subplots(rows=3, cols=2, subplot_titles=('Houston', 'Austin',\n",
    "                                                          'Denver', 'Seattle',\n",
    "                                                          'San Francisco','Portland'))\n",
    "fig.append_trace(trace1, 1, 1)\n",
    "fig.append_trace(trace2, 1, 2)\n",
    "fig.append_trace(trace3, 2, 1)\n",
    "fig.append_trace(trace4, 2, 2)\n",
    "fig.append_trace(trace5, 3, 1)\n",
    "fig.append_trace(trace6, 3, 2)\n",
    "\n",
    "fig['layout'].update(title='Total Precipitation in U.S. Cities (2000-2018)')\n",
    "\n",
    "py.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Cost of Living Considerations\n",
    "\n",
    "We researched effective tax rates for each city using:\n",
    "https://smartasset.com/taxes/oregon-tax-calculator#D3qPPxj0kR\n",
    "\n",
    "We researched comparative cost of living (normalized to $100) for each city using:\n",
    "https://www.nerdwallet.com/cost-of-living-calculator/compare/houston-tx-vs-seattle-wa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Salary Differentials Considerations\n",
    "Different regions of the country provide different salaries: https://datasciencedegree.wisconsin.edu/data-science/data-scientist-salary/\n",
    "https://learning.oreilly.com/library/view/2016-data-science/9781492049029/ch03.html#idm140536273113840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Houston, Austin, Denver, Seattle, San F., Portland\n",
    "mean_data_science_salary = [90,90,80,105,125,105]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model to Predict Whether I'd like a City or Not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data will be modeled using a Logistic Regression Algorithm.\n",
    "\n",
    "The goal is to classify cities I haven't visited based on my previous experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Note: at first I did this analysis with 6 samples or cities. \n",
    "According to some smart people-- Long (1997)-- the minimum number of samples is guided by the formula\n",
    "\n",
    "N = 10k/p,\n",
    "\n",
    "where N is the number of samples, \n",
    "k is the number of independent variables, \n",
    "and p is the smallest proportion of positive / negative cases.\n",
    "\n",
    "N = 10*6/.5 = 120 samples... wow... OK! UHM. \n",
    "\n",
    "It was really difficult to get all that data for those cities. Maybe I need to decrease my independent variables.\n",
    "\n",
    "N = 10*3/.5 = 60 samples -- still a lot\n",
    "\n",
    "Ok I need a break.\n",
    "\n",
    "Ok came back from a long break. This realization came too late and I am going to have to incorporate a more thorough analysis in the future.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some data preparation steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the average summer temperatures for each city between 2000-2018\n",
    "avg_summer_t = [pd.to_numeric(hou_temp_df['Aug Average']).mean(),pd.to_numeric(aus_temp_df['Aug Average']).mean(),\n",
    "                pd.to_numeric(den_temp_df['Aug Average']).mean(),pd.to_numeric(sea_temp_df['Aug Average']).mean(),\n",
    "                pd.to_numeric(san_temp_df['Aug Average']).mean(),pd.to_numeric(por_temp_df['Aug Average']).mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the average winter temperatures for each city between 2000-2018\n",
    "avg_winter_t = [pd.to_numeric(hou_temp_df['Feb Average']).mean(),pd.to_numeric(aus_temp_df['Feb Average']).mean(),\n",
    "                pd.to_numeric(den_temp_df['Feb Average']).mean(),pd.to_numeric(sea_temp_df['Feb Average']).mean(),\n",
    "                pd.to_numeric(san_temp_df['Feb Average']).mean(),pd.to_numeric(por_temp_df['Feb Average']).mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the average annual precipitation for each city between 2000-2018\n",
    "avg_prec = [pd.to_numeric(hou_prec_df['Precipitation']).mean(),pd.to_numeric(aus_prec_df['Precipitation']).mean(),\n",
    "            pd.to_numeric(den_prec_df['Precipitation']).mean(),pd.to_numeric(sea_prec_df['Precipitation']).mean(),\n",
    "            pd.to_numeric(san_prec_df['Precipitation']).mean(),pd.to_numeric(por_prec_df['Precipitation']).mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a dataframe with all features to be used in the classification exercise. This is the original model I created. It had many features and \n",
    "# only a few samples. I learned that I needed to include way more samples if I wanted to use this many features.\n",
    "\n",
    "'''class_df = pd.DataFrame()\n",
    "class_df['City'] = cities\n",
    "class_df['Outdoors'] = outdoors_count_list\n",
    "class_df['Cultural'] = cultural_count_list\n",
    "class_df['Healthy Foods'] = food_count_list\n",
    "class_df['Avg Summer T.'] = avg_summer_t\n",
    "class_df['Avg Winter T.'] = avg_winter_t\n",
    "class_df['Avg Annual Precip.'] = avg_prec\n",
    "class_df['Tax Rates'] = [.34, .34, .35, .32, .38, .37]\n",
    "class_df['Cost of Living'] = [100,105,120,162,201,136]\n",
    "class_df['Mean Salary'] = mean_data_science_salary\n",
    "class_df['Like?'] = [0,1,0,0,1,1]\n",
    "class_df = class_df.round(2)'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead, I chose these three features from the Foursquare Data to run a classification model.\n",
    "# The results were not great, since these did not seem to predict preference well.\n",
    "# Predicting preferences is very tricky since they are often based on subjective biases and opinions.\n",
    "\n",
    "class_df = pd.DataFrame()\n",
    "class_df['City'] = cities\n",
    "class_df['Outdoors'] = outdoors_count_list\n",
    "class_df['Cultural'] = cultural_count_list\n",
    "class_df['Healthy Foods'] = food_count_list\n",
    "class_df['Like?'] = [0,0,0,0,0,0,0,0,0,0,1,0,0,1,1,1,0,0,0,1,0,0,0,0,1,0,1,0,1,0,0,1,1,0,1,0,1,1,1,0,1]\n",
    "class_df = class_df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class_df['Like?'] = class_df['Like?'].astype(int)\n",
    "class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y for this dataset\n",
    "X = np.asarray(class_df[['Outdoors','Cultural']])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.asarray(class_df['Like?'])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 4)\n",
    "print ('Train set:', X_train.shape, y_train.shape)\n",
    "print ('Test set:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling (Logistic Regression with Scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "LR = LogisticRegression(C=0.01, solver='liblinear').fit(X_train,y_train)\n",
    "LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Using Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = LR.predict(X_test)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yhat_prob = LR.predict_proba(X_test)\n",
    "yhat_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "### Evaluation of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Jaccard Index\n",
    "\n",
    "from sklearn.metrics import jaccard_similarity_score\n",
    "jaccard_similarity_score(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "print(confusion_matrix(y_test, yhat, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['like=1','like=0'],normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, yhat_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This log loss value is pretty high. Our model is not doing a good job of classifying which cities we would prefer.\n",
    "\n",
    "In order to improve this model, we would have to extract data from many other cities.\n",
    "\n",
    "We should consider other more prescriptive parameters. However, it's very time consuming to get data for all these cities and the time\n",
    "\n",
    "pressure of the deadline approached. In the near future, we will improve on this model and republish the results in my personal blog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Future Work: Other Datasets to Consider\n",
    "####  Pollen and Mold Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pollen Data Source - Web Scraping Exercise (Complicated)\n",
    "\n",
    "HOUSTON DATA - Station 188\n",
    "http://pollen.aaaai.org/nab/index.cfm?p=AllergenCalendar&stationid=188&qsFullDate=10/1/2018\n",
    "\n",
    "AUSTIN DATA - Station 111 \n",
    "\n",
    "DENVER DATA - Station 196\n",
    "\n",
    "SAN JOSE DATA - Station 108\n",
    "\n",
    "SEATTLE DATA - Station 3\n",
    "\n",
    "PORTLAND DATA - Station 1\n",
    "\n",
    "\n",
    "Mold Spore Count Houston\n",
    "http://www.houstontx.gov/health/Pollen-Mold/mold-archives.html\n",
    "\n",
    "What the Numbers Mean\n",
    "http://www.houstontx.gov/health/Pollen-Mold/numbers.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "url = 'https://www.zyrtec.com/sites/zyrtec_us/files/field/image/pollen-types-by-month-guide.jpg'\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###          Other Sources and Statistics to Consider:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will have to think more deeply about where to find reliable data regarding these statistics and how to integrate them into my analysis: Healthiest US Cities, Best standard of living, cost of living, demographics.\n",
    "\n",
    "This website contains open government data.\n",
    "https://cities.data.gov/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had some success with a limited dataset to predict preferences.\n",
    "Predicting preferences is extremely difficult since there are so many variables and many are subjective.\n",
    "I thought this exercise would come up with a more clear-cut result!\n",
    "\n",
    "Extending this example to other applications, marketers love to segment populations based on preferences, but this is a very complicated \n",
    "exercise. No wonder the tech companies want all the data they can gather about us.\n",
    "\n",
    "It takes 10*(# of independent variables)/(smallest portion of yes/no preference) samples to be able to start having some success with\n",
    "classification algorithms.\n",
    "\n",
    "To improve this exercise, many more samples need to be taken. Therefore, I need to get a job so I am able to start travelling to all\n",
    "these cities!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
